{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f905fa",
   "metadata": {},
   "source": [
    "## FB ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a135c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_id {'unique': 4259, 'most_common': [('4d66f5853f0365dba032a87704a634f023d15babde973bb7a284ed8cd2707b2d', 769176)]}\n",
      "ad_id {'unique': 215756, 'most_common': [('c72a821cf136e9e37d490722482e43d71e18fa51e6076e8991d231b74030b191', 21)]}\n",
      "ad_creation_time {'unique': 544, 'most_common': [('2024-10-27', 118265)]}\n",
      "bylines {'unique': 3766, 'most_common': [('HARRIS FOR PRESIDENT', 627557)]}\n",
      "currency {'unique': 10, 'most_common': [('USD', 3210113)]}\n",
      "delivery_by_region {'unique': 141121, 'most_common': [(\"{'Georgia': {'spend': 49, 'impressions': 499}}\", 28009)]}\n",
      "estimated_audience_size {'count': 3210840, 'mean': 569500.3293764249, 'min': 0.0, 'max': 1000001.0, 'std': 404151.6285794621}\n",
      "estimated_impressions {'count': 3210840, 'mean': 56920.615157404296, 'min': 499.0, 'max': 1000000.0, 'std': 151090.20084232534}\n",
      "estimated_spend {'count': 3210840, 'mean': 1346.9959605586077, 'min': 49.0, 'max': 474999.0, 'std': 5690.148424202872}\n",
      "publisher_platforms {'unique': 9, 'most_common': [(\"['facebook', 'instagram']\", 2806786)]}\n",
      "illuminating_scored_message {'unique': 25579, 'most_common': [('e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', 76467)]}\n",
      "illuminating_mentions {'unique': 275, 'most_common': [('[]', 1003691)]}\n",
      "scam_illuminating {'count': 3210840, 'mean': 0.05374574877602123, 'min': 0.0, 'max': 1.0, 'std': 0.22551531900013566}\n",
      "election_integrity_Truth_illuminating {'count': 3210840, 'mean': 0.04843997209452978, 'min': 0.0, 'max': 1.0, 'std': 0.21469409762174985}\n",
      "advocacy_msg_type_illuminating {'count': 3210840, 'mean': 0.5401131791057792, 'min': 0.0, 'max': 1.0, 'std': 0.49838841300953307}\n",
      "issue_msg_type_illuminating {'count': 3210840, 'mean': 0.37531829676969264, 'min': 0.0, 'max': 1.0, 'std': 0.48420506595788626}\n",
      "attack_msg_type_illuminating {'count': 3210840, 'mean': 0.25925458758455733, 'min': 0.0, 'max': 1.0, 'std': 0.4382256339048913}\n",
      "image_msg_type_illuminating {'count': 3210840, 'mean': 0.22516039416476685, 'min': 0.0, 'max': 1.0, 'std': 0.41768797612576264}\n",
      "cta_msg_type_illuminating {'count': 3210840, 'mean': 0.5684518692927707, 'min': 0.0, 'max': 1.0, 'std': 0.4952922551305133}\n",
      "engagement_cta_subtype_illuminating {'count': 3210840, 'mean': 0.1225760237196497, 'min': 0.0, 'max': 1.0, 'std': 0.3279499590257686}\n",
      "fundraising_cta_subtype_illuminating {'count': 3210840, 'mean': 0.24235838596753498, 'min': 0.0, 'max': 1.0, 'std': 0.4285100417802833}\n",
      "voting_cta_subtype_illuminating {'count': 3210840, 'mean': 0.14100702619875172, 'min': 0.0, 'max': 1.0, 'std': 0.3480288529487516}\n",
      "covid_topic_illuminating {'count': 3210840, 'mean': 0.02455837101817593, 'min': 0.0, 'max': 1.0, 'std': 0.1547748845641842}\n",
      "economy_topic_illuminating {'count': 3210840, 'mean': 0.1285115421509636, 'min': 0.0, 'max': 1.0, 'std': 0.3346585731243845}\n",
      "education_topic_illuminating {'count': 3210840, 'mean': 0.0146245219319555, 'min': 0.0, 'max': 1.0, 'std': 0.1200443658750455}\n",
      "environment_topic_illuminating {'count': 3210840, 'mean': 0.02339138667762953, 'min': 0.0, 'max': 1.0, 'std': 0.15114310047650217}\n",
      "foreign_policy_topic_illuminating {'count': 3210840, 'mean': 0.005474891305701935, 'min': 0.0, 'max': 1.0, 'std': 0.0737896914662559}\n",
      "governance_topic_illuminating {'count': 3210840, 'mean': 0.02435811189595246, 'min': 0.0, 'max': 1.0, 'std': 0.15415836559280494}\n",
      "health_topic_illuminating {'count': 3210840, 'mean': 0.09683914489666255, 'min': 0.0, 'max': 1.0, 'std': 0.29573865515306724}\n",
      "immigration_topic_illuminating {'count': 3210840, 'mean': 0.03501264466619327, 'min': 0.0, 'max': 1.0, 'std': 0.18381177846479815}\n",
      "lgbtq_issues_topic_illuminating {'count': 3210840, 'mean': 0.002788366907102191, 'min': 0.0, 'max': 1.0, 'std': 0.05273132639233753}\n",
      "military_topic_illuminating {'count': 3210840, 'mean': 0.002345803590337731, 'min': 0.0, 'max': 1.0, 'std': 0.0483766630177036}\n",
      "race_and_ethnicity_topic_illuminating {'count': 3210840, 'mean': 0.010865692466768821, 'min': 0.0, 'max': 1.0, 'std': 0.1036707892382547}\n",
      "safety_topic_illuminating {'count': 3210840, 'mean': 0.03294402710817107, 'min': 0.0, 'max': 1.0, 'std': 0.178490134484537}\n",
      "social_and_cultural_topic_illuminating {'count': 3210840, 'mean': 0.0922322507505824, 'min': 0.0, 'max': 1.0, 'std': 0.2893535704772749}\n",
      "technology_and_privacy_topic_illuminating {'count': 3210840, 'mean': 0.0008894868632507381, 'min': 0.0, 'max': 1.0, 'std': 0.029811003893709047}\n",
      "womens_issue_topic_illuminating {'count': 3210840, 'mean': 0.06792615016631162, 'min': 0.0, 'max': 1.0, 'std': 0.2516191725766633}\n",
      "incivility_illuminating {'count': 3210840, 'mean': 0.17420924119545042, 'min': 0.0, 'max': 1.0, 'std': 0.37928937011490765}\n",
      "freefair_illuminating {'count': 3210840, 'mean': 0.005567079019820359, 'min': 0.0, 'max': 1.0, 'std': 0.07440489483356837}\n",
      "fraud_illuminating {'count': 3210840, 'mean': 0.0028142168404529655, 'min': 0.0, 'max': 1.0, 'std': 0.052974502338722064}\n",
      "gender {'count': 2313454, 'mean': 0.4977790783823668, 'min': 0.0, 'max': 1.0, 'std': 0.4999951755453351}\n",
      "age_group {'unique': 8, 'most_common': [('45-54', 565291)]}\n",
      "spend {'count': 3210840, 'mean': 80.63522006702296, 'min': 0.0, 'max': 112121.0, 'std': 540.4059223479069}\n",
      "impressions {'count': 3210840, 'mean': 3499.052319330767, 'min': 0.0, 'max': 1000000.0, 'std': 15307.724660159123}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict, Counter\n",
    "import statistics\n",
    "\n",
    "def load_data(filepath):\n",
    "    with open(filepath, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return list(reader)\n",
    "\n",
    "def analyze_column(values):\n",
    "    numeric_vals = []\n",
    "    for v in values:\n",
    "        try:\n",
    "            numeric_vals.append(float(v))\n",
    "        except:\n",
    "            continue\n",
    "    if numeric_vals:\n",
    "        return {\n",
    "            'count': len(numeric_vals),\n",
    "            'mean': statistics.mean(numeric_vals),\n",
    "            'min': min(numeric_vals),\n",
    "            'max': max(numeric_vals),\n",
    "            'std': statistics.stdev(numeric_vals) if len(numeric_vals) > 1 else 0\n",
    "        }\n",
    "    else:\n",
    "        counter = Counter(values)\n",
    "        return {\n",
    "            'unique': len(counter),\n",
    "            'most_common': counter.most_common(1)\n",
    "        }\n",
    "\n",
    "\n",
    "data = load_data('/Users/pranavdalvi/Research Analyst ischool/Task_03_Descriptive_Stats/facebook_ads_cleaned.csv')\n",
    "columns = data[0].keys()\n",
    "for col in columns:\n",
    "    values = [row[col] for row in data]\n",
    "    print(col, analyze_column(values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb4759",
   "metadata": {},
   "source": [
    "# FB posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d10c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook_Id {'unique': 18, 'most_common': [('32fc18da91029ff09bf74fe9887eace6b5d2145809d583f696e344530508b064', 8303)]}\n",
      "post_id {'unique': 16280, 'most_common': [('8570b69695e00d8f06b12398ed525497e1712b5369c6fc2138fe98f69811c138', 1)]}\n",
      "Page Category {'unique': 5, 'most_common': [('PERSON', 9453)]}\n",
      "Page Admin Top Country {'unique': 1, 'most_common': [('US', 16280)]}\n",
      "Post Created {'unique': 16270, 'most_common': [('2023-11-14 11:11:44 EST', 2)]}\n",
      "Post Created Date {'unique': 335, 'most_common': [('2024-01-10', 99)]}\n",
      "Post Created Time {'unique': 13939, 'most_common': [('19:42:00', 7)]}\n",
      "Type {'unique': 9, 'most_common': [('Link', 7372)]}\n",
      "Total Interactions {'count': 11669, 'mean': 211.2038735110121, 'min': 3.0, 'max': 999.0, 'std': 242.2646145602472}\n",
      "Likes {'count': 16280, 'mean': 1478.9215601965602, 'min': 1.0, 'max': 315973.0, 'std': 8224.84302828365}\n",
      "Comments {'count': 16280, 'mean': 695.0365479115479, 'min': 0.0, 'max': 93872.0, 'std': 3174.510268724084}\n",
      "Shares {'count': 16280, 'mean': 226.1603194103194, 'min': 0.0, 'max': 76150.0, 'std': 1378.6891401941962}\n",
      "Love {'count': 16280, 'mean': 483.1683660933661, 'min': 0.0, 'max': 244482.0, 'std': 4027.405529304152}\n",
      "Wow {'count': 16280, 'mean': 6.85036855036855, 'min': 0.0, 'max': 4345.0, 'std': 57.15433764640952}\n",
      "Haha {'count': 16280, 'mean': 123.40718673218673, 'min': 0.0, 'max': 99276.0, 'std': 1016.8661427963724}\n",
      "Sad {'count': 16280, 'mean': 11.873955773955775, 'min': 0.0, 'max': 56111.0, 'std': 452.0086254609278}\n",
      "Angry {'count': 16280, 'mean': 23.410135135135135, 'min': 0.0, 'max': 11814.0, 'std': 168.3584049679594}\n",
      "Care {'count': 16280, 'mean': 40.7781941031941, 'min': 0.0, 'max': 85236.0, 'std': 853.625066432895}\n",
      "Is Video Owner? {'count': 3170, 'mean': 0.9747634069400631, 'min': 0.0, 'max': 1.0, 'std': 0.15686768322222816}\n",
      "Post Views {'count': 16280, 'mean': 6588.373095823096, 'min': 0.0, 'max': 4276477.0, 'std': 91119.29742341778}\n",
      "Total Views {'count': 16280, 'mean': 7580.581265356265, 'min': 0.0, 'max': 4462155.0, 'std': 96746.81036529384}\n",
      "Total Views For All Crossposts {'count': 16280, 'mean': 3613.601842751843, 'min': 0.0, 'max': 4499458.0, 'std': 88804.32215613915}\n",
      "Video Length {'unique': 766, 'most_common': [('', 13103)]}\n",
      "Overperforming Score {'count': 16280, 'mean': -2.768464987714988, 'min': -198.75, 'max': 246.78, 'std': 7.863076282903143}\n",
      "advocacy_msg_type_illuminating {'count': 16280, 'mean': 0.551965601965602, 'min': 0.0, 'max': 1.0, 'std': 0.4973075180782409}\n",
      "issue_msg_type_illuminating {'count': 16280, 'mean': 0.4754914004914005, 'min': 0.0, 'max': 1.0, 'std': 0.49941430582258417}\n",
      "attack_msg_type_illuminating {'count': 16280, 'mean': 0.2285012285012285, 'min': 0.0, 'max': 1.0, 'std': 0.4198800379473749}\n",
      "image_msg_type_illuminating {'count': 16280, 'mean': 0.15055282555282556, 'min': 0.0, 'max': 1.0, 'std': 0.3576234447014578}\n",
      "cta_msg_type_illuminating {'count': 16280, 'mean': 0.12143734643734644, 'min': 0.0, 'max': 1.0, 'std': 0.3266448701410586}\n",
      "engagement_cta_subtype_illuminating {'count': 16280, 'mean': 0.089004914004914, 'min': 0.0, 'max': 1.0, 'std': 0.28475958302488935}\n",
      "fundraising_cta_subtype_illuminating {'count': 16280, 'mean': 0.01633906633906634, 'min': 0.0, 'max': 1.0, 'std': 0.1267796850472694}\n",
      "voting_cta_subtype_illuminating {'count': 16280, 'mean': 0.01762899262899263, 'min': 0.0, 'max': 1.0, 'std': 0.13160271686183198}\n",
      "covid_topic_illuminating {'count': 16280, 'mean': 0.04858722358722359, 'min': 0.0, 'max': 1.0, 'std': 0.21501010425480194}\n",
      "economy_topic_illuminating {'count': 16280, 'mean': 0.08753071253071253, 'min': 0.0, 'max': 1.0, 'std': 0.28261987396415417}\n",
      "education_topic_illuminating {'count': 16280, 'mean': 0.015171990171990172, 'min': 0.0, 'max': 1.0, 'std': 0.12224041370851087}\n",
      "environment_topic_illuminating {'count': 16280, 'mean': 0.02192874692874693, 'min': 0.0, 'max': 1.0, 'std': 0.1464554352182666}\n",
      "foreign_policy_topic_illuminating {'count': 16280, 'mean': 0.03992628992628992, 'min': 0.0, 'max': 1.0, 'std': 0.1957920733845992}\n",
      "governance_topic_illuminating {'count': 16280, 'mean': 0.03292383292383293, 'min': 0.0, 'max': 1.0, 'std': 0.17844273600954935}\n",
      "health_topic_illuminating {'count': 16280, 'mean': 0.04914004914004914, 'min': 0.0, 'max': 1.0, 'std': 0.2161670071763539}\n",
      "immigration_topic_illuminating {'count': 16280, 'mean': 0.04312039312039312, 'min': 0.0, 'max': 1.0, 'std': 0.20313433839262315}\n",
      "lgbtq_issues_topic_illuminating {'count': 16280, 'mean': 0.0033783783783783786, 'min': 0.0, 'max': 1.0, 'std': 0.058027336373846064}\n",
      "military_topic_illuminating {'count': 16280, 'mean': 0.005343980343980344, 'min': 0.0, 'max': 1.0, 'std': 0.07290918144003719}\n",
      "race_and_ethnicity_topic_illuminating {'count': 16280, 'mean': 0.021805896805896806, 'min': 0.0, 'max': 1.0, 'std': 0.14605379136462948}\n",
      "safety_topic_illuminating {'count': 16280, 'mean': 0.03323095823095823, 'min': 0.0, 'max': 1.0, 'std': 0.17924462376706893}\n",
      "social_and_cultural_topic_illuminating {'count': 16280, 'mean': 0.0636977886977887, 'min': 0.0, 'max': 1.0, 'std': 0.244221301388454}\n",
      "technology_and_privacy_topic_illuminating {'count': 16280, 'mean': 0.002027027027027027, 'min': 0.0, 'max': 1.0, 'std': 0.044978244229468646}\n",
      "womens_issue_topic_illuminating {'count': 16280, 'mean': 0.02622850122850123, 'min': 0.0, 'max': 1.0, 'std': 0.15981907232578044}\n",
      "incivility_illuminating {'count': 16280, 'mean': 0.13237100737100738, 'min': 0.0, 'max': 1.0, 'std': 0.33890408497707103}\n",
      "scam_illuminating {'count': 15331, 'mean': 0.01930728589133129, 'min': 0.0, 'max': 1.0, 'std': 0.1376072299372232}\n",
      "freefair_illuminating {'count': 16280, 'mean': 0.00300982800982801, 'min': 0.0, 'max': 1.0, 'std': 0.05478095726544723}\n",
      "fraud_illuminating {'count': 16280, 'mean': 0.008968058968058967, 'min': 0.0, 'max': 1.0, 'std': 0.0942771384978129}\n",
      "is_crosspost {'count': 16280, 'mean': 0.004422604422604423, 'min': 0.0, 'max': 1.0, 'std': 0.06635748237123373}\n",
      "is_owned {'count': 16280, 'mean': 0.18482800982800981, 'min': 0.0, 'max': 1.0, 'std': 0.38816990079823654}\n",
      "is_share {'count': 16280, 'mean': 0.005896805896805897, 'min': 0.0, 'max': 1.0, 'std': 0.07656626982498803}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict, Counter\n",
    "import statistics\n",
    "\n",
    "def load_data(filepath):\n",
    "    with open(filepath, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return list(reader)\n",
    "\n",
    "def analyze_column(values):\n",
    "    numeric_vals = []\n",
    "    for v in values:\n",
    "        try:\n",
    "            numeric_vals.append(float(v))\n",
    "        except:\n",
    "            continue\n",
    "    if numeric_vals:\n",
    "        return {\n",
    "            'count': len(numeric_vals),\n",
    "            'mean': statistics.mean(numeric_vals),\n",
    "            'min': min(numeric_vals),\n",
    "            'max': max(numeric_vals),\n",
    "            'std': statistics.stdev(numeric_vals) if len(numeric_vals) > 1 else 0\n",
    "        }\n",
    "    else:\n",
    "        counter = Counter(values)\n",
    "        return {\n",
    "            'unique': len(counter),\n",
    "            'most_common': counter.most_common(1)\n",
    "        }\n",
    "\n",
    "\n",
    "data = load_data('/Users/pranavdalvi/Research Analyst ischool/Task_03_Descriptive_Stats/facebook_posts_cleaned.csv')\n",
    "columns = data[0].keys()\n",
    "for col in columns:\n",
    "    values = [row[col] for row in data]\n",
    "    print(col, analyze_column(values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8617d68c",
   "metadata": {},
   "source": [
    "# Twitter Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc5b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id {'unique': 27304, 'most_common': [('cc46051622b8a9c1b883a3bbf12c640b12ac1cbdc7f48a773b6cc2a65f03aa2d', 1)]}\n",
      "url {'unique': 27304, 'most_common': [('f70a206472e9deaf6e313297c1efb891729ced346a0aeb34e16935d78f74b937', 1)]}\n",
      "source {'unique': 14, 'most_common': [('Twitter Web App', 14930)]}\n",
      "retweetCount {'count': 27304, 'mean': 1322.0551933782597, 'min': 0.0, 'max': 144615.0, 'std': 3405.0042401645187}\n",
      "replyCount {'count': 27304, 'mean': 1063.7850131848813, 'min': 0.0, 'max': 121270.0, 'std': 3174.981654139348}\n",
      "likeCount {'count': 27304, 'mean': 6913.69282888954, 'min': 0.0, 'max': 915221.0, 'std': 21590.307989209447}\n",
      "quoteCount {'count': 27304, 'mean': 128.08156314093173, 'min': 0.0, 'max': 123320.0, 'std': 1131.5334680019284}\n",
      "viewCount {'count': 27304, 'mean': 507084.7318341635, 'min': 5.0, 'max': 333502775.0, 'std': 3212173.9862966556}\n",
      "createdAt {'unique': 25106, 'most_common': [('16/09/24 21:47', 26)]}\n",
      "lang {'unique': 12, 'most_common': [('en', 27281)]}\n",
      "bookmarkCount {'count': 27304, 'mean': 136.21352182830356, 'min': 0.0, 'max': 42693.0, 'std': 712.5802944831519}\n",
      "isReply {'count': 27304, 'mean': 0.1235716378552593, 'min': 0.0, 'max': 1.0, 'std': 0.3290982449542024}\n",
      "isRetweet {'count': 27304, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std': 0.0}\n",
      "isQuote {'count': 27304, 'mean': 0.11866393202461178, 'min': 0.0, 'max': 1.0, 'std': 0.32339856788807175}\n",
      "isConversationControlled {'count': 27304, 'mean': 0.0002929973630237328, 'min': 0.0, 'max': 1.0, 'std': 0.017114971333590297}\n",
      "month_year {'unique': 15, 'most_common': [('2024-10', 3586)]}\n",
      "illuminating_scored_message {'unique': 27136, 'most_common': [('36cb7d55fcf85362ca03f624c2f574f1f55f89db559b17da084df6e643afe5cd', 21)]}\n",
      "election_integrity_Truth_illuminating {'count': 26034, 'mean': 0.03714373511561804, 'min': 0.0, 'max': 1.0, 'std': 0.1891175609378361}\n",
      "advocacy_msg_type_illuminating {'count': 26034, 'mean': 0.5636859491434278, 'min': 0.0, 'max': 1.0, 'std': 0.4959370396322804}\n",
      "issue_msg_type_illuminating {'count': 26034, 'mean': 0.507682261657832, 'min': 0.0, 'max': 1.0, 'std': 0.4999505813425477}\n",
      "attack_msg_type_illuminating {'count': 26034, 'mean': 0.3075977567795959, 'min': 0.0, 'max': 1.0, 'std': 0.46150791760416376}\n",
      "image_msg_type_illuminating {'count': 26034, 'mean': 0.22643466236460014, 'min': 0.0, 'max': 1.0, 'std': 0.41853164098392054}\n",
      "cta_msg_type_illuminating {'count': 26034, 'mean': 0.10966428516555274, 'min': 0.0, 'max': 1.0, 'std': 0.31247684757947275}\n",
      "engagement_cta_subtype_illuminating {'count': 26034, 'mean': 0.0669124990397173, 'min': 0.0, 'max': 1.0, 'std': 0.24987519849406606}\n",
      "fundraising_cta_subtype_illuminating {'count': 26034, 'mean': 0.007874318199277867, 'min': 0.0, 'max': 1.0, 'std': 0.08838898916084756}\n",
      "voting_cta_subtype_illuminating {'count': 26034, 'mean': 0.016785741722363065, 'min': 0.0, 'max': 1.0, 'std': 0.1284702866854094}\n",
      "covid_topic_illuminating {'count': 26034, 'mean': 0.0076054390412537455, 'min': 0.0, 'max': 1.0, 'std': 0.08687857194055204}\n",
      "economy_topic_illuminating {'count': 26034, 'mean': 0.16021356687408772, 'min': 0.0, 'max': 1.0, 'std': 0.3668110523395825}\n",
      "education_topic_illuminating {'count': 26034, 'mean': 0.01843742797879696, 'min': 0.0, 'max': 1.0, 'std': 0.13452949268938846}\n",
      "environment_topic_illuminating {'count': 26034, 'mean': 0.028539602058846123, 'min': 0.0, 'max': 1.0, 'std': 0.16651173583618067}\n",
      "foreign_policy_topic_illuminating {'count': 26034, 'mean': 0.042252439118076364, 'min': 0.0, 'max': 1.0, 'std': 0.20116839951473553}\n",
      "governance_topic_illuminating {'count': 26034, 'mean': 0.022969962356917877, 'min': 0.0, 'max': 1.0, 'std': 0.1498105645777889}\n",
      "health_topic_illuminating {'count': 26034, 'mean': 0.05565798571099332, 'min': 0.0, 'max': 1.0, 'std': 0.2292644615290389}\n",
      "immigration_topic_illuminating {'count': 26034, 'mean': 0.06529922409157256, 'min': 0.0, 'max': 1.0, 'std': 0.2470578473918854}\n",
      "lgbtq_issues_topic_illuminating {'count': 26034, 'mean': 0.0030729046631328264, 'min': 0.0, 'max': 1.0, 'std': 0.05534961243011968}\n",
      "military_topic_illuminating {'count': 26034, 'mean': 0.010985634170699855, 'min': 0.0, 'max': 1.0, 'std': 0.10423707289416256}\n",
      "race_and_ethnicity_topic_illuminating {'count': 26034, 'mean': 0.015402934623953292, 'min': 0.0, 'max': 1.0, 'std': 0.12315139782021392}\n",
      "safety_topic_illuminating {'count': 26034, 'mean': 0.03760467081508796, 'min': 0.0, 'max': 1.0, 'std': 0.19024181908327997}\n",
      "social_and_cultural_topic_illuminating {'count': 26034, 'mean': 0.05197050011523392, 'min': 0.0, 'max': 1.0, 'std': 0.22197175454151633}\n",
      "technology_and_privacy_topic_illuminating {'count': 26034, 'mean': 0.0020357993393254974, 'min': 0.0, 'max': 1.0, 'std': 0.04507474794044172}\n",
      "womens_issue_topic_illuminating {'count': 26034, 'mean': 0.02331566413152032, 'min': 0.0, 'max': 1.0, 'std': 0.15090698683369944}\n",
      "incivility_illuminating {'count': 26034, 'mean': 0.17857417223630637, 'min': 0.0, 'max': 1.0, 'std': 0.3830027047455516}\n",
      "scam_illuminating {'count': 26034, 'mean': 0.012368441269109626, 'min': 0.0, 'max': 1.0, 'std': 0.11052570813895939}\n",
      "freefair_illuminating {'count': 27304, 'mean': 0.0014283621447406974, 'min': 0.0, 'max': 1.0, 'std': 0.037767369074478356}\n",
      "fraud_illuminating {'count': 27304, 'mean': 0.0027468502783474947, 'min': 0.0, 'max': 1.0, 'std': 0.05233932958795364}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict, Counter\n",
    "import statistics\n",
    "\n",
    "def load_data(filepath):\n",
    "    with open(filepath, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        return list(reader)\n",
    "\n",
    "def analyze_column(values):\n",
    "    numeric_vals = []\n",
    "    for v in values:\n",
    "        try:\n",
    "            numeric_vals.append(float(v))\n",
    "        except:\n",
    "            continue\n",
    "    if numeric_vals:\n",
    "        return {\n",
    "            'count': len(numeric_vals),\n",
    "            'mean': statistics.mean(numeric_vals),\n",
    "            'min': min(numeric_vals),\n",
    "            'max': max(numeric_vals),\n",
    "            'std': statistics.stdev(numeric_vals) if len(numeric_vals) > 1 else 0\n",
    "        }\n",
    "    else:\n",
    "        counter = Counter(values)\n",
    "        return {\n",
    "            'unique': len(counter),\n",
    "            'most_common': counter.most_common(1)\n",
    "        }\n",
    "\n",
    "\n",
    "data = load_data('/Users/pranavdalvi/Research Analyst ischool/Task_03_Descriptive_Stats/twitter_posts_cleaned.csv')\n",
    "columns = data[0].keys()\n",
    "for col in columns:\n",
    "    values = [row[col] for row in data]\n",
    "    print(col, analyze_column(values))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
